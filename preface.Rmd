# Preface {-}

Various books have been written for learning Apache Spark; for instance, [Spark: The Definitive Guide: Big Data Processing Made Simple](@information-technology) is a comprehensive resource while [Learning Spark: Lightning-Fast Big Data Analysis](@spark-learning-karau) is an introductory book meant to help users get up an running. However, as of this writting, there is no book to learn Apache Spark using the R programming language and neither, a book specifically designed for the R user nor the aspiring R user.

There are some resources online to learn Apache Spark with R, most notably, the [spark.rstudio.com](https://spark.rstudio.com) site and the Spark documentation site under [spark.apache.org](https://spark.apache.org/docs/latest/index.html). Both sites are great online resources; however, the content is not intended to be read from start to finish and assumes the reader has some context on Apache Spark, R and cluster computing.

The goal of this book is to help anyone get started with Apache Spark using R, with the first chapters being mostly introductory, but quickly ramping up to relevant data science topics, cluster computing and closing on advanced topics which should interst even the most advanced users.

This book is inteded to be a useful resource for a wide range of users; from those of you curious to learn the tools used in **big data** and **big compute**, to those of you experienced in those topics seeking to understand deeper topics while working with Apache Spark from R.

## Structure {-}

This book has the following general outline:

1. **Introductions**: In the first chapters, **Introduction** and **Getting Started**, you will learn about Apache Spark, R and the tools you will need to perform data analysis with Spark and R.
2. **Analysis**: In the **Analysis** chapter, you will learn how to analyse, explore, transform and visualize data in Apache Spark with R.
3. **Modeling**: In the **Modeling** chapter, you will learn how to create statistical models with the purpose of extracting information and predictions outcomes.
4. **Scaling**: Up to this point, chapters will have focused on performing operations on your personal computer; the **Clusters**, **Connections**, **Data** and **Tuning** chapters, introduce distributed dcomputing techniques required to perform analysis and modeling across many machines to tackle the large-scale data and computation problems that Apache Spark was designed for.
5. **Extensions**: The extension chapter describe optional components and extended functionality applicable to specific, yet relevant, use cases. You will learn about alternative modeling frameworks, graph processing at scale and model deployment topics that will be relevant to many readers at some point in time.
6. **Advanced Topics**: This book closes with a set of advanced chapters, **Distributed R**, **Streaming** and **Contributing**, which the advanced users will be most interested in. However, by the time you reach this section, these chapters won't seem as intimidating; instead, they will be equally relevant, useful and interesting as the previous chapters.

## Authors {-}

**Javier Luraschi**

Javier is a Software Engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked for Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering.

**Kevin Kuo**

Kevin is a software engineer working on open source packages for big data analytics and machine learning. He has held data science positions in a variety of industries and was a credentialed actuary. He likes mixing cocktails and studying about wine.

**Edgar Ruiz**

Edgar has a background in deploying enterprise reporting and Business Intelligence solutions. He has posted multiple articles and blog posts sharing analytics insights and server infrastructure for Data Science. He lives with his family near Biloxi, MS.

## Acknowledgments {-}

This project would not have been possible without the work put into building **sparklyr** by Javier Luraschi, Kevin Kuo, Kevin Ushey and JJ Allaire, nor without **bookdown** by Yihui Xie, **dplyr** by Hadley Wickham, **DBI** by Kirill MÃ¼lller nor the **Apache Spark** project iteself.
